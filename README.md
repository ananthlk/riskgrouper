# Risk Grouper Project

## Overview
This project develops a risk stratification model to predict the likelihood of emergency department (ED) and inpatient (IP) visits within 30, 60, and 90-day periods. The model leverages a combination of claims data (medical, behavioral, and pharmacy), care team notes, and operational data to generate predictions.

The entire data pipeline is built using Snowflake SQL, with Python scripts for orchestration, model training, and utility functions.

## Key Features
- **Point-in-Time Correct Architecture**: The data pipeline is designed to prevent data leakage by respecting the natural data lags from different sources (e.g., 4 months for medical claims, 2 months for pharmacy). All features for a given member-day are built using only historically available data.
- **Thematic, Condition-Specific Labels**: In addition to general ED/IP visit predictions, the model predicts risk for seven key clinical conditions: HIV, Malnutrition, Serious Mental Illness (SMI), Congestive Heart Failure (CHF), COPD, Substance Use Disorder (SUD), and Diabetes.
- **Modular SQL Pipeline**: The data preparation is split into logical, sequential SQL scripts that build on each other, with dedicated validation scripts for each step.
- **ML Model**: An XGBoost model is used for prediction, trained on the features generated by the SQL pipeline.

## Project Structure
```
.
├── docs/
├── scripts/
│   ├── analysis/
│   │   └── top_conditions_analysis.sql
│   ├── sql/
│   │   ├── events.sql
│   │   └── daily_aggregation.sql
│   ├── utils/
│   │   ├── run_analysis.py
│   │   ├── run_sql_script.py
│   │   └── get_table_def.py
│   └── validation/
│       ├── validate_events.sql
│       └── validate_daily_aggregation.sql
├── src/
│   ├── RiskGrouper.py
│   └── snowflake_connector.py
├── .env
├── .gitignore
├── main.py
├── README.md
└── requirements.txt
```

## Setup and Installation
1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```
2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
3.  **Install the required Python packages:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Configure Environment Variables:**
    Create a `.env` file in the root directory and add your Snowflake credentials.
    ```
    SNOWFLAKE_USER=your_user
    SNOWFLAKE_PASSWORD=your_password
    SNOWFLAKE_ACCOUNT=your_account
    SNOWFLAKE_WAREHOUSE=your_warehouse
    SNOWFLAKE_DATABASE=your_database
    SNOWFLAKE_SCHEMA=your_schema
    ```

## Usage
1.  **Run the full SQL pipeline:**
    The SQL scripts must be run in order. Use the `run_analysis.py` utility to execute them.
    ```bash
    # Step 1: Create the base events table
    python scripts/utils/run_analysis.py --sql-file scripts/sql/events.sql

    # Step 2: Run validation on the events table
    python scripts/utils/run_analysis.py --sql-file scripts/validation/validate_events.sql

    # Step 3: Create the final aggregated features table
    python scripts/utils/run_analysis.py --sql-file scripts/sql/daily_aggregation.sql

    # Step 4: Run validation on the final table
    python scripts/utils/run_analysis.py --sql-file scripts/validation/validate_daily_aggregation.sql
    ```
2.  **Train the Model:**
    Execute the main Python script to train the model and generate predictions using the data from the SQL pipeline.
    ```bash
    python src/RiskGrouper.py
    ```

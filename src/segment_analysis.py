"""
Model Performance Segmentation Analysis

This script provides a command-line tool to analyze the performance of a trained
machine learning model across various demographic and clinical segments of a population.
It takes a prediction file (generated by a script like `RiskGrouper.py`) as input,
which should contain true labels, predicted labels, and predicted probabilities, along
with various feature columns to be used for segmentation.

Key Features:
- **Command-Line Interface**: Allows users to specify the input prediction file and the
  target variable via command-line arguments for flexible, automated analysis.
- **Multi-Segment Analysis**: Automatically identifies and analyzes performance across
  several predefined segments:
    - Engagement Group
    - Coverage Category
    - Gender
    - Clinical conditions (e.g., Diabetes, Mental Health, Cardiovascular)
- **Robust Metrics Calculation**: For each segment, it calculates key performance metrics:
    - Record Count and Positive Rate
    - Accuracy
    - AUC (Area Under the ROC Curve)
    - Precision, Recall, and F1-score for the positive class.
- **Graceful Error Handling**:
    - Checks for the existence of the input file and required columns.
    - Handles cases where a segment might have only one class (e.g., all negatives),
      preventing crashes during metric calculation.
- **Organized Output**: Saves the detailed segmentation analysis to a new CSV file,
  named based on the input file, for easy review and further analysis.
- **Logging**: Creates a timestamped log file to capture the entire process, aiding
  in debugging and tracking.

Usage:
    Run the script from the command line, providing the path to the prediction file
    and the name of the target variable.

    $ python src/segment_analysis.py "output/Master_Dataset_Analysis_20231027_103000/y_any_90d_predictions.csv" --target "y_any_90d"

"""
import pandas as pd
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
import os
import argparse
import logging
from datetime import datetime

def setup_logger(analysis_name):
    """
    Sets up a dedicated logger for the segmentation analysis run.

    Args:
        analysis_name (str): The name for the analysis, used in the log file name.

    Returns:
        logging.Logger: A configured logger instance.
    """
    log_dir = 'logs'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        
    log_filename = datetime.now().strftime(f'{analysis_name.replace(" ", "_")}_%Y-%m-%d_%H-%M-%S.log')
    log_filepath = os.path.join(log_dir, log_filename)

    logger = logging.getLogger(f"{analysis_name}-{datetime.now().strftime('%H%M%S')}")
    logger.setLevel(logging.INFO)

    # Create handlers
    file_handler = logging.FileHandler(log_filepath)
    console_handler = logging.StreamHandler()

    # Create formatters and add it to handlers
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # Add handlers to the logger, preventing duplicates
    if not logger.handlers:
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

    return logger

def analyze_segment_performance(val_df, target, base_output, logger):
    """
    Analyzes and reports model performance across different population segments.

    This function iterates through predefined segmentation columns, calculates performance
    metrics for each segment value, and saves the results to a CSV file.

    Args:
        val_df (pd.DataFrame): The DataFrame containing validation data with true labels and predictions.
        target (str): The name of the target variable column.
        base_output (str): The base path for the output CSV file.
        logger (logging.Logger): The logger instance for logging messages.
    """
    logger.info("--- Analyzing Model Performance by Segment ---")
    
    # Validate that the necessary columns exist in the DataFrame.
    if target not in val_df.columns:
        logger.error(f"Target column '{target}' not found in the dataframe. Please specify a valid target column.")
        return
    if 'predicted_label' not in val_df.columns or 'predicted_proba' not in val_df.columns:
        logger.error("Prediction columns ('predicted_label', 'predicted_proba') not found. Cannot perform analysis.")
        return

    # Define segmentation schemes. The script will check if these columns exist.
    segments = {}
    if 'engagement_group' in val_df.columns:
        segments['engagement_group'] = val_df['engagement_group'].unique()
    if 'normalized_coverage_category' in val_df.columns:
        segments['normalized_coverage_category'] = val_df['normalized_coverage_category'].unique()
    if 'gender' in val_df.columns:
        segments['gender'] = val_df['gender'].unique()
    
    # Define binary clinical flags based on count columns.
    clinical_flags = {
        'has_diabetes': 'cnt_hcc_diabetes',
        'has_mental_health': 'cnt_hcc_mental_health',
        'has_cardiovascular': 'cnt_hcc_cardiovascular'
    }
    for flag_name, col_name in clinical_flags.items():
        if col_name in val_df.columns:
            segments[flag_name] = [True, False]

    results = []

    # Iterate through each segmentation column and its unique values.
    for segment_column, values in segments.items():
        for value in values:
            # Filter the DataFrame to create the specific segment.
            if segment_column.startswith('has_'):
                col_name = clinical_flags[segment_column]
                condition = val_df[col_name] > 0
                if not value:  # Handle the 'False' case (does not have the condition)
                    condition = ~condition
                segment_df = val_df[condition]
            else:
                segment_df = val_df[val_df[segment_column] == value]

            if len(segment_df) == 0:
                continue

            y_true = segment_df[target]
            y_pred = segment_df['predicted_label']
            y_pred_proba = segment_df['predicted_proba']
            
            # Handle cases where a segment contains only one class (e.g., all negatives).
            if len(y_true.unique()) < 2:
                auc = float('nan')  # AUC is not defined for a single class.
                report = {}
                accuracy = accuracy_score(y_true, y_pred)
            else:
                auc = roc_auc_score(y_true, y_pred_proba)
                report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
                accuracy = accuracy_score(y_true, y_pred)

            # Append the calculated metrics to the results list.
            results.append({
                'segment_column': segment_column,
                'segment_value': value,
                'record_count': len(segment_df),
                'positive_rate': y_true.mean(),
                'accuracy': accuracy,
                'auc': auc,
                'precision_pos': report.get('1.0', {}).get('precision', 0),
                'recall_pos': report.get('1.0', {}).get('recall', 0),
                'f1_score_pos': report.get('1.0', {}).get('f1-score', 0),
            })

    if not results:
        logger.warning("No segments were analyzed. Skipping report generation.")
        return

    # Create a DataFrame from the results and save it to a CSV file.
    performance_df = pd.DataFrame(results)
    performance_path = os.path.splitext(base_output)[0] + "_segment_performance.csv"
    performance_df.to_csv(performance_path, index=False)
    logger.info(f"Segment performance analysis saved to {performance_path}")

def main():
    """
    Main function to parse command-line arguments and run the segmentation analysis.
    """
    # Set up an argument parser to handle command-line inputs.
    parser = argparse.ArgumentParser(description="Run segmentation analysis on model predictions.")
    parser.add_argument("input_file", type=str, help="Path to the prediction CSV file from RiskGrouper.")
    parser.add_argument("--target", type=str, required=True, help="Name of the target variable column (e.g., y_any_90d).")
    args = parser.parse_args()

    logger = setup_logger("SegmentationAnalysis")
    
    # Check if the specified input file exists.
    if not os.path.exists(args.input_file):
        logger.error(f"Input file not found: {args.input_file}")
        return

    logger.info(f"Loading predictions from: {args.input_file}")
    try:
        df = pd.read_csv(args.input_file)
        # Standardize column names to lower case for consistency.
        df.columns = [col.lower() for col in df.columns]
    except Exception as e:
        logger.error(f"Failed to read input file: {e}")
        return

    # Call the main analysis function.
    analyze_segment_performance(df, args.target.lower(), args.input_file, logger)
    logger.info("Segmentation analysis complete.")

if __name__ == "__main__":
    main()
